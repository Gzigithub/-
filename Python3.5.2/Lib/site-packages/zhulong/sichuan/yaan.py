
import pandas as pd
import re

from lxml import etree
from selenium import webdriver
from bs4 import BeautifulSoup
from lmf.dbv2 import db_write,db_command,db_query
from selenium.webdriver import DesiredCapabilities
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException,StaleElementReferenceException
from selenium.common.exceptions import WebDriverException
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

import sys
import time

import json


# __conp=["postgres","since2015","192.168.3.171","hunan","changsha"]


# url="https://ggzy.changsha.gov.cn/spweb/CS/TradeCenter/tradeList.do?Deal_Type=Deal_Type2"
# driver=webdriver.Chrome()
# driver.minimize_window()
# driver.get(url)

from zhulong.util.etl import add_info,est_meta,est_html,est_tbs


_name_="yaan"


def f2_data(driver, num):
    try:
        locator = (By.XPATH, '//div[@class="new_news_content"]/ul/li[1]//a')
        val = WebDriverWait(driver, 10).until(EC.presence_of_element_located(locator)).text.strip()
    except:
        val = None
        locator = (By.XPATH, '//div[@class="new_news_content"]/ul/li[1]/span[1]')
        val_1 = WebDriverWait(driver, 10).until(EC.presence_of_element_located(locator)).text.strip()
    try:
        locator = (By.XPATH, "//td[@class='page_bar']")
        str = WebDriverWait(driver, 10).until(EC.presence_of_element_located(locator)).text.strip()
        cnum = re.findall(r'(\d+)/', str)[0]
    except:
        cnum = 1
    if num != int(cnum):
        locator = (By.XPATH, '//*[@id="txtAjax_PageIndex"]')
        WebDriverWait(driver, 10).until(EC.presence_of_element_located(locator)).clear()

        locator = (By.XPATH, '//*[@id="txtAjax_PageIndex"]')
        WebDriverWait(driver, 10).until(EC.presence_of_element_located(locator)).send_keys(num)
        locator = (By.XPATH, '//*[@id="tbPager"]/tbody/tr/td[1]/span[3]')
        WebDriverWait(driver, 10).until(EC.presence_of_element_located(locator)).click()
        if val != None:
            locator = (By.XPATH, "//div[@class='new_news_content']/ul/li[1]//a[not(contains(string(), '%s'))]" % val)
            WebDriverWait(driver, 10).until(EC.presence_of_element_located(locator))
        else:
            locator = (By.XPATH, "//div[@class='new_news_content']/ul/li[1]/span[1][not(contains(string(), '%s'))]" % val_1)
            WebDriverWait(driver, 10).until(EC.presence_of_element_located(locator))

    page = driver.page_source
    soup = BeautifulSoup(page, 'html.parser')
    table = soup.find("div", class_="new_news_content")
    trs = table.find_all("li")
    data = []
    for tr in trs:
        try:
            a = tr.find('a')
            try:
                title = a['title'].strip()
            except:
                title = a.text.strip()
            href = a['href'].strip()
            if "http" in href:
                link = href
            else:
                link = "http://ggzy.yazw.gov.cn:8007" + a['href'].strip()
            try:
                span = tr.find('span', class_="time")
                if span == None:
                    span = tr.find('span', class_="tiam")
                span = span.text.strip()
            except:
                span = '-'
            try:
                tds = re.findall(r'\[(.*?)\]', title)
                s = tds[0] if tds else None
            except:
                s = None
            dq = {"diqu": s}
            info = json.dumps(dq, ensure_ascii=False)
            try:
                title = re.sub(r'\[(.*?)\]', '', title)
            except:
                title = title
        except:
            title = tr.find_all('span')[0].text.strip()
            link = '-'
            span = '-'
            try:
                tds = re.findall(r'\[(.*?)\]', title)
                dd = tds[0] if tds else None
            except:
                dd = None
            s = tr.find('span', class_="time").text.strip()
            yy = {"yuanyin": s, "diqu": dd}
            info = json.dumps(yy, ensure_ascii=False)
            try:
                title = re.sub(r'\[(.*?)\]', '', title)
            except:
                title = title

        tmp = [title, span, link, info]
        data.append(tmp)
        # print(tmp)
        # d = f3(driver, link)
        # print(d)
    df = pd.DataFrame(data)
    return df


def f3_data(driver, num):
    url = driver.current_url
    page = driver.page_source
    soup = BeautifulSoup(page, 'html.parser')
    table = soup.find("table")
    trs = table.find_all("tr")
    data = []
    for tr in trs:
        # print(tr)
        try:
            title = tr.find_all('td')[1]
            title = title['title'].strip()
        except:
            title = '-'
        try:
            td = tr.find_all('td')[2].text.strip()
            td = re.findall(r'(\d+-\d+-\d+)', td)[0]
        except:
            td = '-'

        link = '-'
        try:
            span = tr.find_all('td')[3].text.strip()
            a={"yuanyin":span}
            a=json.dumps(a,ensure_ascii=False)
            info=a
        except:
            info = None
        tmp = [title, td, link, info]
        data.append(tmp)
        # print(tmp)
        # d = f3(driver, link)
        # print(d)
    df = pd.DataFrame(data)
    return df


def f1(driver, num):
    url = driver.current_url
    if 'http://ggzy.yazw.gov.cn:8007/JyWeb/' in url:
        df = f2_data(driver, num)
        return df
    elif ('http://ggzy.yazw.gov.cn/ceinwz/cxzbxm_first.aspx' in url) or ('http://ggzy.yazw.gov.cn/ceinwz/cxzbxmEnsure_first.aspx' in url):
        df = f3_data(driver, num)
        return df
    locator = (By.XPATH, '//*[@id="ctl00_ContentPlaceHolder1_myGV_ctl02_HLinkGcmc"]')
    val = WebDriverWait(driver, 10).until(EC.presence_of_element_located(locator)).text
    try:
        locator = (By.XPATH, '//tr[@class="myGVPagerCss"]/td/span[1]')
        str = WebDriverWait(driver, 10).until(EC.presence_of_element_located(locator)).text.strip()
        cnum = int(str)
    except:
        cnum = 1
    url = driver.current_url
    if num != int(cnum):
        if num > int(cnum):
            t = num - int(cnum)
            for i in range(t):
                locator = (By.XPATH, '//*[@id="ctl00_ContentPlaceHolder1_myGV_ctl02_HLinkGcmc"]')
                WebDriverWait(driver, 10).until(EC.presence_of_element_located(locator))
                driver.find_element_by_link_text('下一页').click()

            locator = (By.XPATH, '//tr[@class="myGVPagerCss"]/td/span[1]')
            cnum_1 = WebDriverWait(driver, 10).until(EC.presence_of_element_located(locator)).text.strip()
            if num != int(cnum_1):
                raise TimeoutError

        if num < int(cnum):
            t = int(cnum) - num
            for i in range(t):
                locator = (By.XPATH, '//*[@id="ctl00_ContentPlaceHolder1_myGV_ctl02_HLinkGcmc"]')
                WebDriverWait(driver, 10).until(EC.presence_of_element_located(locator))
                driver.find_element_by_link_text('上一页').click()

            locator = (By.XPATH, '//tr[@class="myGVPagerCss"]/td/span[1]')
            cnum_1 = WebDriverWait(driver, 10).until(EC.presence_of_element_located(locator)).text.strip()
            if num != int(cnum_1):
                raise TimeoutError

    page = driver.page_source

    soup = BeautifulSoup(page, 'html.parser')

    table = soup.find("table", id='ctl00_ContentPlaceHolder1_myGV')

    trs = table.find_all("tr")
    data = []
    for tr in trs[1:-1]:
        a = tr.find('a')
        try:
            title = a['title'].strip()
        except:
            title = a.text.strip()

        link = "http://ggzy.yazw.gov.cn/ceinwz/" + a['href'].strip()
        try:
            td = tr.find("td", class_="fFbDate")
            span = td.find('span').text.strip()
        except:
            span = '-'
        try:
            title = re.sub(r'\[(.*)\]', '', title)
            if '※' in title:
                title = re.sub(r'※', '', title).strip()
        except:
            if '※' in title:
                title = re.sub(r'※', '', title).strip()
            title = title

        tmp = [title, span, link]
        data.append(tmp)
        # print(tmp)
        # d = f3(driver, link)
        # print(d)
    df = pd.DataFrame(data)
    df['info'] = None
    return df


def f2(driver):
    url = driver.current_url
    if 'http://ggzy.yazw.gov.cn:8007/JyWeb' in url:
        locator = (By.XPATH, '//div[@class="new_news_content"]/ul/li[1]//a | //div[@class="new_news_content"]/ul/li[1]/span[1]')
        WebDriverWait(driver, 10).until(EC.presence_of_element_located(locator))
        try:
            locator = (By.XPATH, "//td[@class='page_bar']")
            str = WebDriverWait(driver, 10).until(EC.presence_of_element_located(locator)).text.strip()
            num = re.findall(r'/(\d+)', str)[0]
        except:
            num = 1
        driver.quit()
        return int(num)

    elif ('http://ggzy.yazw.gov.cn/ceinwz/cxzbxm_first.aspx' in url) or ('http://ggzy.yazw.gov.cn/ceinwz/cxzbxmEnsure_first.aspx' in url):
        num = 1
        driver.quit()
        return int(num)

    else:
        locator = (By.XPATH, '//*[@id="ctl00_ContentPlaceHolder1_myGV_ctl02_HLinkGcmc"]')
        WebDriverWait(driver, 10).until(EC.presence_of_element_located(locator))
        try:
            locator = (By.XPATH, '//*[@id="ctl00_ContentPlaceHolder1_myGV_ctl23_LabelPageCount"]')
            str = WebDriverWait(driver, 10).until(EC.presence_of_element_located(locator)).text.strip()
            num = int(str)
        except:
            num = 1

        driver.quit()
        return int(num)



def f3(driver, url):
    driver.get(url)
    time.sleep(1)
    html = driver.page_source
    if ("您请求的文件不存在" in html) or ("找不到文件" in html) or ("Not Found" in html):
        return
    if 'http://ggzy.yazw.gov.cn:8016/' in url:
        # driver.get(url)
        locator = (By.XPATH, "//div[@class='xxym']")
        WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located(locator))

        before = len(driver.page_source)
        time.sleep(0.1)
        after = len(driver.page_source)
        i = 0
        while before != after:
            before = len(driver.page_source)
            time.sleep(0.1)
            after = len(driver.page_source)
            i += 1
            if i > 5: break

        page = driver.page_source

        soup = BeautifulSoup(page, 'html.parser')

        div = soup.find('div', class_="page_contect bai_bg")
        if div == None:
            div = soup.find('div', class_="xxym")
        return div

    elif 'http://ggzy.yazw.gov.cn:8007/' in url:
        # driver.get(url)
        locator = (By.XPATH, "//div[@class='mid_content']")
        WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located(locator))

        before = len(driver.page_source)
        time.sleep(0.1)
        after = len(driver.page_source)
        i = 0
        while before != after:
            before = len(driver.page_source)
            time.sleep(0.1)
            after = len(driver.page_source)
            i += 1
            if i > 5: break

        page = driver.page_source

        soup = BeautifulSoup(page, 'html.parser')

        div = soup.find('div', class_="main_con")
        return div

    elif '/ceinwz/admin_show.aspx' in url:
        # driver.get(url)
        locator = (By.XPATH, "//div[@class='newsImage']")
        WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located(locator))
        before = len(driver.page_source)
        time.sleep(0.1)
        after = len(driver.page_source)
        i = 0
        while before != after:
            before = len(driver.page_source)
            time.sleep(0.1)
            after = len(driver.page_source)
            i += 1
            if i > 5: break
        page = driver.page_source
        soup = BeautifulSoup(page, 'html.parser')
        div = soup.find('span', id="ctl00_ContentPlaceHolder1_BodyLabel")
        return div

    else:
        # driver.get(url)
        locator = (By.XPATH, "//table[@class='yygl'] | //div[@class='warpper'] | //div[@class='container']")
        WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located(locator))
        before = len(driver.page_source)
        time.sleep(0.1)
        after = len(driver.page_source)
        i = 0
        while before != after:
            before = len(driver.page_source)
            time.sleep(0.1)
            after = len(driver.page_source)
            i += 1
            if i > 5: break

        page = driver.page_source
        soup = BeautifulSoup(page, 'html.parser')
        div = soup.find('table', width="75%")
        if div == None:
            div = soup.find('div', class_='cont-info')
            if div == None:
                div = soup.find('div', id="myPrintArea")
        # div=div.find_all('div',class_='ewb-article')[0]
        return div

data = [
    ["gcjs_zhaobiao_gg",
     "http://ggzy.yazw.gov.cn/ceinwz/WebInfo_List.aspx?newsid=100&jsgc=0100000&zfcg=&tdjy=&cqjy=&qtjy=&PubDateSort=0&ShowPre=0&CbsZgys=0&zbfs=&qxxx=0&showqxname=0&NewsShowPre=1&wsjj=0&showCgr=0&ShowOverDate=0&showdate=1&FromUrl=jsgc",
     ["name", "ggstart_time", "href", "info"],f1,f2],

    ["gcjs_zhongbiaohx_gg",
     "http://ggzy.yazw.gov.cn/ceinwz/WebInfo_List.aspx?newsid=102&jsgc=0000010&zfcg=&tdjy=&cqjy=&qtjy=&PubDateSort=0&ShowPre=0&CbsZgys=0&zbfs=&qxxx=0&showqxname=0&NewsShowPre=1&wsjj=0&showCgr=0&ShowOverDate=0&showdate=1&FromUrl=jsgc",
     ["name", "ggstart_time", "href", "info"], f1, f2],

    ["gcjs_zhongbiao_gg",
     "http://ggzy.yazw.gov.cn/ceinwz/WebInfo_List.aspx?newsid=103&jsgc=0000001&zfcg=&tdjy=&cqjy=&qtjy=&PubDateSort=0&ShowPre=0&CbsZgys=0&zbfs=&qxxx=0&showqxname=0&NewsShowPre=1&wsjj=0&showCgr=0&ShowOverDate=0&showdate=1&FromUrl=jsgc",
     ["name", "ggstart_time", "href", "info"], f1, f2],

    ["zfcg_zhaobiao_gg",
     "http://ggzy.yazw.gov.cn:8007/JyWeb/XXGK/JYXYZFCGXXFBList?SubType=2&SubType2=2010&Type=%E9%87%87%E8%B4%AD%E4%BF%A1%E6%81%AF",
    ["name", "ggstart_time", "href", "info"],f1,f2],

    ["zfcg_biangen_gg",
     "http://ggzy.yazw.gov.cn:8007/JyWeb/XXGK/JYXYZFCGXXFBList?SubType=2&SubType2=2030&Type=%E9%87%87%E8%B4%AD%E4%BF%A1%E6%81%AF",
     ["name", "ggstart_time", "href", "info"], f1, f2],

    ["zfcg_jieguo_gg",
     "http://ggzy.yazw.gov.cn:8007/JyWeb/XXGK/JYXYZFCGXXFBList?SubType=2&SubType2=2050&Type=%E9%87%87%E8%B4%AD%E4%BF%A1%E6%81%AF",
     ["name", "ggstart_time", "href", "info"], f1, f2],

    ["zfcg_liubiao_gg",
     "http://ggzy.yazw.gov.cn:8007/JyWeb/XXGK/JYXTCXZBXMList?SubType=2&SubType2=2070&Type=%E9%87%87%E8%B4%AD%E4%BF%A1%E6%81%AF",
     ["name", "ggstart_time", "href", "info"], f1, f2],

    ["zfcg_zhaobiao_jinjia_gg",
     "http://ggzy.yazw.gov.cn:8007/JyWeb/TradeInfo/JingJiaXinXiList?SubType=50000&SubType2=6010&Type=%E7%AB%9E%E4%BB%B7%E4%BF%A1%E6%81%AF",
     ["name", "ggstart_time", "href", "info"], f1, f2],

    ["zfcg_zhigou_jinjia_gg",
     "http://ggzy.yazw.gov.cn:8007/JyWeb/TradeInfo/JingJiaXinXiList?SubType=50000&SubType2=6020&Type=%E7%AB%9E%E4%BB%B7%E4%BF%A1%E6%81%AF",
     ["name", "ggstart_time", "href", "info"], f1, f2],

    ["zfcg_zhongbiao_jinjia_gg",
     "http://ggzy.yazw.gov.cn:8007/JyWeb/TradeInfo/JingJiaXinXiList?SubType=50000&SubType2=6030&Type=%E7%AB%9E%E4%BB%B7%E4%BF%A1%E6%81%AF",
     ["name", "ggstart_time", "href", "info"], f1, f2],

    ["zfcg_liubiao_jinjia_gg",
     "http://ggzy.yazw.gov.cn:8007/JyWeb/TradeInfo/JingJiaXinXiList?SubType=50000&SubType2=6040&Type=%E7%AB%9E%E4%BB%B7%E4%BF%A1%E6%81%AF",
     ["name", "ggstart_time", "href", "info"], f1, f2],

    ["zfcg_zhongzhi_jinjia_gg",
     "http://ggzy.yazw.gov.cn:8007/JyWeb/TradeInfo/JingJiaXinXiList?SubType=50000&SubType2=6050&Type=%E7%AB%9E%E4%BB%B7%E4%BF%A1%E6%81%AF",
     ["name", "ggstart_time", "href", "info"], f1, f2],

    ["zfcg_jinjia_gg",
     "http://ggzy.yazw.gov.cn:8007/JyWeb/TradeInfo/JingJiaXinXiList?SubType=50000&SubType2=6060&Type=%E7%AB%9E%E4%BB%B7%E4%BF%A1%E6%81%AF",
     ["name", "ggstart_time", "href", "info"], f1, f2],

    ["gcjs_liubiao1_gg",
     "http://ggzy.yazw.gov.cn/ceinwz/cxzbxmEnsure_first.aspx?num=10000&len=20&FromUrl=cxzb&more=1",
     ["name", "ggstart_time", "href", "info"], f1, f2],

    ["gcjs_liubiao2_gg",
     "http://ggzy.yazw.gov.cn/ceinwz/cxzbxm_first.aspx?num=10000&len=20&FromUrl=cxzb&more=1",
     ["name", "ggstart_time", "href", "info"], f1, f2],

]



def work(conp,**args):
    est_meta(conp,data=data,diqu="四川省雅安市",**args)
    est_html(conp,f=f3,**args)


if __name__=='__main__':
    work(conp=["postgres","since2015","192.168.3.171","sichuan","yaan"],pageloadtimeout=60,pageLoadStrategy="none")



    # driver = webdriver.Chrome()
    # url = "http://ggzy.yazw.gov.cn:8007/JyWeb/TradeInfo/JingJiaXinXiList?SubType=50000&SubType2=6020&Type=%E7%AB%9E%E4%BB%B7%E4%BF%A1%E6%81%AF"
    # driver.get(url)
    # df = f2(driver)
    # print(df)
    #
    # driver=webdriver.Chrome()
    # url = "http://ggzy.yazw.gov.cn:8007/JyWeb/TradeInfo/JingJiaXinXiList?SubType=50000&SubType2=6020&Type=%E7%AB%9E%E4%BB%B7%E4%BF%A1%E6%81%AF"
    # driver.get(url)
    # for i in range(15, 16):
    #     df=f1(driver, i)
    #     print(df)
