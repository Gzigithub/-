import pandas as pd  
import re 

from selenium import webdriver 
from bs4 import BeautifulSoup
from lmf.dbv2 import db_write,db_command,db_query
from selenium.webdriver.common.keys import Keys 
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException,StaleElementReferenceException
from selenium.common.exceptions import WebDriverException
from selenium.webdriver.support.wait import WebDriverWait 
from selenium.webdriver.support import expected_conditions as EC 

import sys 
import time

import json
from zhulong.util.etl import gg_meta,gg_html,est_meta,est_html


_name_="loudi"

# driver=webdriver.Chrome()

# url="""http://ldggzy.hnloudi.gov.cn/Trading/main/ViewsPage.aspx?code=D4313002018000397&type=8&pid=5#title_8"""

# driver.get(url)



def f1(driver,num):
    

    url=driver.current_url


    url=re.sub("p[0-9]*.aspx",'p%d.aspx'%num,url)

    typeid=int(re.findall("&type=([0-9]*)",url)[0])


    driver.get(url)

    locator=(By.XPATH,"//span[@class='currentPageNo'][string()='%d']"%num)
    WebDriverWait(driver,10).until(EC.presence_of_element_located(locator))

    locator=(By.CLASS_NAME,"list")
    WebDriverWait(driver,10).until(EC.presence_of_element_located(locator))

    page=driver.page_source

    soup=BeautifulSoup(page,"lxml")
    ul=soup.find("ul",class_="list")

    lis=ul.find_all("li")
    data=[]

    for li in lis:
        spans=li.find_all("span")
        a=li.find("a")
        name=a.text.strip()
        href="http://ldggzy.hnloudi.gov.cn"+a["href"]
        if typeid==2:
            href=href.replace("type=0&pid=2#title_0","type=2&pid=1#title_2")
        elif typeid==3:
            href=href.replace("type=0&pid=2#title_0","type=3&pid=1#title_3")
        elif typeid==4:
            href=href.replace("type=0&pid=2#title_0","type=4&pid=1#title_4")
        else:
            href=href

        ggstart_time=spans[1].text.strip()
        info=json.dumps({"xmtype":spans[0].text.strip()},ensure_ascii=False)
        tmp=[name,href,ggstart_time,info]
        data.append(tmp)
    df=pd.DataFrame(data=data)

    return df 

def f2(driver):

    locator=(By.CLASS_NAME,"turnpage")
    WebDriverWait(driver,10).until(EC.presence_of_element_located(locator))

    total=int(driver.find_element_by_xpath("//div[@class='turnpage']//a[last()-1]").text)

    driver.quit()

    return total

def f3(driver,url):


    driver.get(url)

    code=re.findall("code=(.*)&type",url)[0]
    locator=(By.XPATH,"//iframe[contains(@src,'%s')]"%code)

    WebDriverWait(driver,10).until(EC.presence_of_all_elements_located(locator))
    driver.switch_to.frame(driver.find_element_by_xpath("//iframe[contains(@src,'%s')]"%code))
    before=len(driver.page_source)
    time.sleep(0.1)
    after=len(driver.page_source)
    i=0
    while before!=after:
        before=len(driver.page_source)
        time.sleep(0.1)
        after=len(driver.page_source)
        i+=1
        if i>5:break

    page=driver.page_source

    soup=BeautifulSoup(page,'lxml')

    div=soup.find('div',class_='content')

    #div=div.find_all('div',class_='ewb-article')[0]
    
    return div

data=[
["gcjs_zhaobiao_gg","http://ldggzy.hnloudi.gov.cn/Trading/main/list/p2.aspx?id=2&type=2",["name","href","ggstart_time","info"] ,f1,f2  ],

["gcjs_qita_gg","http://ldggzy.hnloudi.gov.cn/Trading/main/list/p2.aspx?id=2&type=3",["name","href","ggstart_time","info"] ,f1,f2  ],

["gcjs_zhongbiao_gg","http://ldggzy.hnloudi.gov.cn/Trading/main/list/p2.aspx?id=2&type=4",["name","href","ggstart_time","info"] ,f1,f2  ],

["zfcg_zhaobiao_gg","http://ldggzy.hnloudi.gov.cn/Trading/main/list/p2.aspx?id=5&type=6",["name","href","ggstart_time","info"] ,f1,f2  ],

["zfcg_qita_gg","http://ldggzy.hnloudi.gov.cn/Trading/main/list/p2.aspx?id=5&type=7",["name","href","ggstart_time","info"] ,f1,f2  ],

["zfcg_zhongbiao_gg","http://ldggzy.hnloudi.gov.cn/Trading/main/list/p2.aspx?id=5&type=8",["name","href","ggstart_time","info"] ,f1,f2  ],

]

#=["postgres","since2015","127.0.0.1","hunan","loudi"]
def work(conp,**args):
    est_meta(conp,data=data,diqu="湖南省娄底市",**args)

    est_html(conp,f=f3,**args)


# work(conp=["postgres","since2015","127.0.0.1","hunan","loudi"])
